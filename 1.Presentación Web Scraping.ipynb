{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n"
   ]
  },
  {
   "attachments": {
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una técnica que sirve para extraer información de páginas web de forma **automática** así podemos obtener grandes volumenes de información de diferentes sitios.\n",
    "\n",
    "\n",
    "   **RECOLECCIÓN** -> ALMACENAMIENTO -> TRATAMIENTO -> VISUALIZACIÓN \n",
    "                        \n",
    "                        \n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "\n",
    "Minería de datos \n",
    "\n",
    "Monitoreo de cambio de precio en línea y comparación de precios\n",
    "\n",
    "Monitoreo de datos meteorológicos  \n",
    "\n",
    "Integración de datos web\n",
    "\n",
    "Recopilación de listados de bienes raíces\n",
    "\n",
    "** ... **\n",
    "\n",
    "                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "                               'Nombre','apellido','autorID','articulos','indice-h'\n",
    "                                Juan D.,  Velasquez,26649280600,1,1\n",
    "                                Carlos Jaime,Franco,7103081268,1,1\n",
    "                                Jackeline D.,Perez,56583222900,1,1\n",
    "                                Carlos A.,Martinez,57192164250,1,1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beautiful Soup ##   \n",
    "  \n",
    "*\"Beautiful Soup is a Python library for pulling data out of HTML and XML files.\"* \n",
    "\n",
    "\n",
    "[Documentación](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) de Beautiful Soup\n",
    "\n",
    "\n",
    "\n",
    "    pip install beautifulsoup4\n",
    "    pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingrese su usuario SIN el @unal.edu.co: adtamayop\n",
      "ingrese su contraseña\n",
      "········\n",
      "El Login fue correcto \n",
      "\n",
      "['\"Juan D.\"', '\"Velasquez\"', '26649280600', '1', '1']\n",
      "['\"Juan D.\"', '\"Velasquez\"', '26649280600', '64', '5']\n",
      "['\"Carlos Jaime\"', '\"Franco\"', '7103081268', '1', '1']\n",
      "['\"Carlos Jaime\"', '\"Franco\"', '7103081268', '53', '7']\n",
      "['\"Jackeline D.\"', '\"Perez\"', '56583222900', '1', '1']\n",
      "['\"Jackeline D.\"', '\"Perez\"', '56583222900', '8', '5']\n",
      "['\"Carlos A.\"', '\"Martinez\"', '57192164250', '1', '1']\n",
      "['\"Carlos A.\"', '\"Martinez\"', '57192164250', '5', '2']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup       # libreria para analizar doc HTML (arbol con todos los elementos)\n",
    "from requests import session        # “post”, “get”, “put”, “patch”, “delete”, “head” y “options”  RESTFUL, persistir solicitud\n",
    "import getpass                      #*******\n",
    "\n",
    "\n",
    "username = input('Ingrese su usuario SIN el @unal.edu.co: ')\n",
    "print('ingrese su contraseña')\n",
    "password = getpass.getpass()\n",
    "datos = {'user': username,'pass': password,'submit':'Ingresar'}\n",
    "\n",
    "\n",
    "with session() as c:  \n",
    "    \n",
    "    while (True):\n",
    "        \n",
    "        #paso los parámetros a la página de logueo, y luego obtengo la página inicial de scopus\n",
    "        c.post('https://login.ezproxy.unal.edu.co/login', data=datos)\n",
    "        \n",
    "        main_page = c.get('https://login.ezproxy.unal.edu.co/login?url=http://www.scopus.com/')\n",
    "        \n",
    "        if main_page.url != 'https://www-scopus-com.ezproxy.unal.edu.co/search/form.uri?display=basic':\n",
    "            print('Ha ocurrido un error')\n",
    "        else:\n",
    "            print('El Login fue correcto \\n')\n",
    "            break    \n",
    "            \n",
    "    \n",
    "    profesores = open(\"ws/profesores.csv\").readlines()\n",
    "    profesores = [x.replace('\\n','') for x in profesores]\n",
    "    profesores = [x.split(',') for x in profesores]\n",
    "        \n",
    "    with open('ws/profesores.csv','w',encoding=\"utf-8\") as file:\n",
    "        \n",
    "        #escribimos la cabecera del archivo profesores\n",
    "        file.write(','.join(profesores[0]))\n",
    "        file.write('\\n')\n",
    "            \n",
    "        for x in range(1,len(profesores)):\n",
    "            \n",
    "            profesor_page = c.get(\"https://www-scopus-com.ezproxy.unal.edu.co/authid/detail.uri?origin=AuthorProfile&authorId=\"+str(profesores[x][2])+\"&zone=\")\n",
    "            \n",
    "                                                  #html.parser,lxml  estructura en la que vamos a representar el árbol\n",
    "            file_soup = BeautifulSoup(profesor_page.content,\"lxml\")  #pasamos el contenido de la página a un Objeto BeatifulSoup()\n",
    "                        \n",
    "            #print(file_soup.prettify())\n",
    "                       \n",
    "                            \n",
    "            results =  file_soup.findAll('div',{'class':'panel-body'})  #div-> etiqueta que define el bloque, panel-body --> class name \n",
    "                        \n",
    "            \"\"\"\n",
    "            results[0]\n",
    "                <div class=\"panel-body\">\n",
    "                    <span class=\"fontLarge\">5</span>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "            \n",
    "            hindex = results[0].find('span').get_text()\n",
    "            \n",
    "            \"\"\"\n",
    "            results[1]\n",
    "            \n",
    "            <div class=\"panel-body\">\n",
    "                <span class=\"fontLarge pull-left\">64</span>\n",
    "                <button class=\"btn btn-sm btn-link color pull-right noPadding\" onclick=\"submitAuthorFormForClickedLinks('HirschSingleAuthorEvaluatorButton');\" title=\"View charts and graphs related to this author\" type=\"button\">\n",
    "                    <span class=\"btnText\">Analyze author output</span>\n",
    "                </button>\n",
    "            </div>\n",
    "\n",
    "            \"\"\"                       \n",
    "            articulos = results[1].find('span').get_text()\n",
    "\n",
    "            print(profesores[x])\n",
    "            \n",
    "            #sustitución\n",
    "            profesores[x][3] = articulos\n",
    "            profesores[x][4] = hindex\n",
    "\n",
    "            print(profesores[x])        \n",
    "         \n",
    "            file.write(','.join(profesores[x]))\n",
    "            file.write('\\n')\n",
    "            \n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium\n",
    "\n",
    "[Documentación](http://selenium-python.readthedocs.io/) de Selenium para Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium Web Driver es una herramienta creada principalmente para la automatización de pruebas o tareas diarias sobre aplicaciones web, pues permite abrir el navegador y llevar a cabo tareas que un humano debería de hacer tales como:\n",
    "hacer click en botones,Ingresar información en formularios, Buscar información específica en una página web, pero a la vez es una potente herramienta de web scraping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pip install selenium\n",
    "\n",
    "[PhantomJS](http://phantomjs.org/download.html)\n",
    "\n",
    "[ChromeDrive](https://sites.google.com/a/chromium.org/chromedriver/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingrese su usuario SIN el @unal.edu.co: adtamayop\n",
      "Ingrese su contraseña: \n",
      "········\n",
      "['\"Juan D.\"', '\"Velasquez\"', '26649280600', '1', '1']\n",
      "['\"Juan D.\"', '\"Velasquez\"', '26649280600', '64', '5']\n",
      "['\"Carlos Jaime\"', '\"Franco\"', '7103081268', '53', '7']\n",
      "['\"Carlos Jaime\"', '\"Franco\"', '7103081268', '53', '7']\n",
      "['\"Jackeline D.\"', '\"Perez\"', '56583222900', '8', '5']\n",
      "['\"Jackeline D.\"', '\"Perez\"', '56583222900', '8', '5']\n",
      "['\"Carlos A.\"', '\"Martinez\"', '57192164250', '5', '2']\n",
      "['\"Carlos A.\"', '\"Martinez\"', '57192164250', '5', '2']\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import getpass\n",
    "\n",
    "#librerías necesarias para hacer un Wait explícito en las peticiones a la pág, esto con el fin de darle un tiempo de carga\n",
    "#a los elementos dinámicos que apenas van cargando, y no tener que colocar un Time.Sleep() , pues aveces puede cargar más ligero\n",
    "#como en otras ocasiones cargar más lento, y sacar errores o excepciones al no encontrar el elemento deseado\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "import time\n",
    "\n",
    "username = input('Ingrese su usuario SIN el @unal.edu.co: ')\n",
    "print('Ingrese su contraseña: ')\n",
    "password = getpass.getpass()\n",
    "\n",
    "#pagina = webdriver.PhantomJS(r\"C:\\Users\\andre\\Google Drive\\Synced\\PAE\\Librería\\phantomjs-2.1.1-windows\\bin\\phantomjs.exe\")\n",
    "pagina = webdriver.Chrome(r\"C:\\Users\\andre\\Google Drive\\Synced\\PAE\\Librería\\chromedriver.exe\")\n",
    "         \n",
    "pagina.get(\"https://login.ezproxy.unal.edu.co/login?url=https://www.scopus.com\")\n",
    "\n",
    "espera = WebDriverWait(pagina,20)\n",
    "\n",
    "success = False\n",
    "\n",
    "while not success:\n",
    "\n",
    "    try:\n",
    "                  \n",
    "        usuario = pagina.find_element_by_name(\"user\")\n",
    "        contraseña = pagina.find_element_by_name(\"pass\")\n",
    "        \n",
    "        usuario.send_keys(username)\n",
    "        contraseña.send_keys(password)\n",
    "                                                        \n",
    "        login_attempt = espera.until(EC.presence_of_element_located((By.XPATH,\"/html/body/div[1]/form/input[2]\")))\n",
    "        #login_attempt.submit()\n",
    "        login_attempt.click()\n",
    "        \n",
    "        success = True\n",
    "\n",
    "    except TimeoutException:\n",
    "        pagina.get(\"https://login.ezproxy.unal.edu.co/login?url=https://www.scopus.com\")\n",
    "\n",
    "        \n",
    "profesores = open(\"ws/profesores.csv\").readlines()\n",
    "profesores = [x.replace('\\n','') for x in profesores]\n",
    "profesores = [x.split(',') for x in profesores]\n",
    "        \n",
    "with open('ws/profesores.csv','w') as file:\n",
    "        \n",
    "    file.write(','.join(profesores[0]))\n",
    "    file.write('\\n')\n",
    "    \n",
    "    for x in range(1,len(profesores)):\n",
    "        \n",
    "        pagina.get('https://www-scopus-com.ezproxy.unal.edu.co/authid/detail.uri?origin=resultslist&authorId=' + str((profesores[x][2])) + '&zone=')\n",
    "          \n",
    "        #time.sleep(3)\n",
    "\n",
    "        success = False\n",
    "        while not success:\n",
    "            \n",
    "            try:\n",
    "                                                                                                      \n",
    "                numero_articulos_autor = int(espera.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"authorDetailsDocumentsByAuthor\"]/div/div[2]/span'))).text)\n",
    "                indice_h_autor = int(espera.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"authorDetailsHindex\"]/div/div[2]/span'))).text)\n",
    "                \n",
    "                print(profesores[x])   \n",
    "                    \n",
    "                profesores[x][3] = str(numero_articulos_autor)\n",
    "                profesores[x][4] = str(indice_h_autor)\n",
    "\n",
    "                print(profesores[x])      \n",
    "                  \n",
    "                file.write(','.join(profesores[x]))\n",
    "                file.write('\\n')\n",
    "                \n",
    "                success = True\n",
    "                \n",
    "            except (TimeoutException,NoSuchElementException):\n",
    "\n",
    "                pagina.get('https://www-scopus-com.ezproxy.unal.edu.co/authid/detail.uri?origin=resultslist&authorId='+str(id)+'&zone=')\n",
    "                time.sleep(3)\n",
    "                \n",
    "pagina.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
